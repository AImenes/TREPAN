{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "859a2020",
   "metadata": {},
   "source": [
    "# TREPAN algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1587a98",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The TREPAN algorithm was introduced in the 1995 paper, \"Extracting Tree-Structured Representations of Trained Networks\" by M. Craven and colleagues. This innovative algorithm was designed with the purpose of enabling analysis of decision-making processes in neural networks or other trained models, and then presenting those findings in a manner that's easily understandable by humans.\n",
    "\n",
    "One such human-friendly mode of presenting complex information is through Decision Trees, which offer a graphical and intuitive way of representing intricate processes. The appeal of decision trees lies in their logical structure that simplifies the interpretation of multifaceted domains.\n",
    "\n",
    "The TREPAN algorithm seeks to leverage this intuitive simplicity; given a trained network and the dataset it was trained on, the goal of TREPAN is to create a concept description that is not only easily comprehensible but also classifies instances in a manner consistent with the network. In essence, TREPAN transforms the complex inner workings of a trained model into a transparent, accessible, and human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d836293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "from itertools import combinations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from graphviz import Digraph\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b5148",
   "metadata": {},
   "source": [
    "## Our trained model we want to analyze\n",
    "We want to train a ANN model using the Iris Dataset for our exploration of the TREPAN algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b38121",
   "metadata": {},
   "source": [
    "## Our Dataset - The Iris Dataset\n",
    "\n",
    "\n",
    "![Iris Flowers](./iris-dataset.png)\n",
    "\n",
    "The **Iris dataset**, also referred to as Fisher's Iris dataset, is a multivariate dataset introduced by the British statistician, eugenicist, and biologist Ronald Fisher in his 1936 paper \"The use of multiple measurements in taxonomic problems\". It is arguably one of the most famous datasets in the field of Machine Learning, often used for testing out classification algorithms.\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The dataset contains **150 observations** of iris flowers from three different species - *Setosa*, *Versicolour*, and *Virginica*. Each observation includes **four features** representing the physical dimensions (in cm) of the flower:\n",
    "\n",
    "1. Sepal Length\n",
    "2. Sepal Width\n",
    "3. Petal Length\n",
    "4. Petal Width\n",
    "\n",
    "The objective when using this dataset is typically to build a model that can predict the species of the flower based on these four features.\n",
    "\n",
    "## Data Source\n",
    "\n",
    "The Iris dataset is publicly available and is included in the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/iris).\n",
    "\n",
    "## Our implementation\n",
    "\n",
    "In our implementation, we will use the Iris Dataset. We will create a vanilla feed forward neural network with two fully connected layers (nn.Linear) together with a ReLU activation function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67ae13ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network model for the Iris dataset classification task.\n",
    "\n",
    "    This class extends the PyTorch `nn.Module` and implements a simple feedforward neural network\n",
    "    with a single hidden layer for the Iris dataset classification. It includes methods for training,\n",
    "    fitting, and predicting using the neural network.\n",
    "\n",
    "    Attributes:\n",
    "        fc1 (nn.Linear): First fully connected layer, mapping input features to hidden dimensions.\n",
    "        relu (nn.ReLU): Rectified Linear Unit activation function.\n",
    "        fc2 (nn.Linear): Second fully connected layer, mapping hidden dimensions to output dimensions.\n",
    "        softmax (nn.Softmax): Softmax activation function for converting logits to probabilities.\n",
    "\n",
    "    Example:\n",
    "        iris_nn = IrisNN(input_dim=4, hidden_dim=10, output_dim=3)\n",
    "        iris_nn.fit(X, y, epochs=100, batch_size=16, lr=0.01)\n",
    "        y_pred = iris_nn.predict(X)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(IrisNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_dim, out_features=16)\n",
    "        self.fc2 = nn.Linear(in_features=16, out_features=12)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(in_features=12, out_features=output_dim)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def fit(self, X, y, epochs=10, batch_size=20, lr=0.01): \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train = torch.tensor(X, dtype=torch.float32)\n",
    "        y_train = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "        # Create data loaders\n",
    "        train_data = TensorDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Define the loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            for data, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.forward(data)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Print loss every 10 epochs\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Convert input to PyTorch tensor\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(X)\n",
    "            _, predicted_labels = torch.max(output, 1)\n",
    "\n",
    "        return predicted_labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea21cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.0877560377120972\n",
      "Epoch: 10, Loss: 0.22161638736724854\n",
      "Epoch: 20, Loss: 0.09254917502403259\n",
      "Epoch: 30, Loss: 0.04215478152036667\n",
      "Accuracy: 1.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Train ANN to represent with TREPAN\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "model_path = \"iris_model.pkl\"\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check if the model already exists\n",
    "if os.path.exists(model_path):\n",
    "    # Load the existing model\n",
    "    model = torch.load(model_path)\n",
    "else:\n",
    "    # Train a new model\n",
    "    model = IrisNN(4, 3)\n",
    "    model.fit(X_train, y_train, epochs=40)\n",
    "\n",
    "    # Save the trained model\n",
    "    # torch.save(model, model_path)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualize the confusion matrix using a heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02939382",
   "metadata": {},
   "source": [
    "## The algorithm\n",
    "\n",
    "![](./trepan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b0c41f",
   "metadata": {},
   "source": [
    "### The Oracle\n",
    "In TREPAN, we define the term **oracle**. The Oracle is the agent between our TREPAN building process and the trained model we want to analyze. We can **query** the oracle to achieve more training data, as well as predicting the labels on the data.\n",
    "\n",
    "The oracle is used for three different purposes: \n",
    "* to determine the class labels for the network's training examples; \n",
    "* to select splits for each of the tree's internal nodes; and\n",
    "* and to determine if a node covers instances of only one class. These aspects of the algorithm are discussed in more detail below. \n",
    "\n",
    "#### Instance generation\n",
    "For continuous data, we use the Kernel Density Function to map the training data distribution. We use this model thereafter to generate more instances.\n",
    "\n",
    "For discrete data, we use a simple frequency count, and use these counts as weights for generation.\n",
    "\n",
    "Instance generation is a non-deterministic manner, and is the reason we obtain different models each time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcb91319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Oracle class\n",
    "class Oracle:\n",
    "    \"\"\"\n",
    "    This is the oracle class used to bind the tree expansion and neural network together. We use this to generate more instances of values, model feature distribution and to keep track of the neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, X, y):\n",
    "        self.model = model\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.discrete_features, self.continuous_features = self._separate_features_by_type(self.X)\n",
    "        self.discrete_distributions = self._model_discrete_features()\n",
    "        self.continuous_kdes = self._model_continuous_features()\n",
    "\n",
    "    def _separate_features_by_type(self, X, discrete_threshold=0.1):\n",
    "        discrete_features = []\n",
    "        continuous_features = []\n",
    "\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        elif not isinstance(X, np.ndarray):\n",
    "            raise ValueError(\"X should be either a pandas DataFrame or a numpy array.\")\n",
    "            \n",
    "        num_instances, num_features = X.shape\n",
    "\n",
    "        for col in range(num_features):\n",
    "            unique_values = np.unique(X[:, col])\n",
    "            num_unique_values = len(unique_values)\n",
    "            \n",
    "            if (np.issubdtype(X[:, col].dtype, np.number) and num_unique_values / num_instances <= discrete_threshold) or not np.issubdtype(X[:, col].dtype, np.number):\n",
    "                discrete_features.append(X[:, col])\n",
    "            else:\n",
    "                continuous_features.append(X[:, col])\n",
    "\n",
    "        if len(discrete_features) > 0:\n",
    "            discrete_features = np.column_stack(discrete_features)\n",
    "        else:\n",
    "            discrete_features = np.empty((num_instances, 0))\n",
    "\n",
    "        if len(continuous_features) > 0:\n",
    "            continuous_features = np.column_stack(continuous_features)\n",
    "        else:\n",
    "            continuous_features = np.empty((num_instances, 0))\n",
    "\n",
    "        return discrete_features, continuous_features\n",
    "\n",
    "    def _model_discrete_features(self):\n",
    "        distributions = []\n",
    "        for col in range(self.discrete_features.shape[1]):\n",
    "            counts = np.bincount(self.discrete_features[:, col].astype(int))\n",
    "            distributions.append(counts / counts.sum())\n",
    "        return distributions\n",
    "\n",
    "    def _model_continuous_features(self):\n",
    "        kdes = []\n",
    "        for col in range(self.continuous_features.shape[1]):\n",
    "            kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(self.continuous_features[:, col].reshape(-1, 1))\n",
    "            kdes.append(kde)\n",
    "        return kdes\n",
    "\n",
    "    def generate_instances(self, hard_constraints, current_split, num_instances):\n",
    "        new_instances = []\n",
    "        instances_created = 0\n",
    "\n",
    "        while instances_created < num_instances:\n",
    "            new_instance = []\n",
    "\n",
    "            # Generate discrete features. Note: Non-deterministic\n",
    "            for col, distribution in enumerate(self.discrete_distributions):\n",
    "                value = np.random.choice(len(distribution), p=distribution)\n",
    "                new_instance.append(value)\n",
    "\n",
    "            # Generate continuous features. Note: Non-deterministic\n",
    "            for col, kde in enumerate(self.continuous_kdes):\n",
    "                value = round(kde.sample()[0][0], 1)\n",
    "                new_instance.append(value)\n",
    "\n",
    "            # Check if the generated instance satisfies all the constraints\n",
    "            satisfies_constraints = True\n",
    "\n",
    "            if hard_constraints:\n",
    "                for constraint in hard_constraints:\n",
    "                    m = constraint.m\n",
    "                    conditions = constraint.conditions\n",
    "                    satisfied_conditions_count = 0\n",
    "\n",
    "                    for feature_idx, threshold, direction in conditions:\n",
    "                        if direction == \"<=\" and new_instance[feature_idx] <= threshold:\n",
    "                            satisfied_conditions_count += 1\n",
    "                        if direction == \">\" and new_instance[feature_idx] > threshold:\n",
    "                            satisfied_conditions_count += 1\n",
    "\n",
    "                    if constraint.satisfied and satisfied_conditions_count < m:\n",
    "                        satisfies_constraints = False\n",
    "                        break\n",
    "                    elif not constraint.satisfied and satisfied_conditions_count >= m:\n",
    "                        satisfies_constraints = False\n",
    "                        break\n",
    "\n",
    "            if satisfies_constraints and current_split:\n",
    "                # Check if the generated instance satisfies the current split\n",
    "                m = current_split.m\n",
    "                conditions = current_split.conditions\n",
    "                satisfied_conditions_count = 0\n",
    "\n",
    "                for feature_idx, threshold, direction in conditions:\n",
    "                    if direction == \"<=\" and new_instance[feature_idx] <= threshold:\n",
    "                        satisfied_conditions_count += 1\n",
    "                    if direction == \">\" and new_instance[feature_idx] > threshold:\n",
    "                        satisfied_conditions_count += 1\n",
    "\n",
    "                if current_split.satisfied and satisfied_conditions_count >= m:\n",
    "                    new_instances.append(new_instance)\n",
    "                    instances_created += 1\n",
    "                elif not current_split.satisfied and satisfied_conditions_count < m:\n",
    "                    new_instances.append(new_instance)\n",
    "                    instances_created += 1\n",
    "            elif satisfies_constraints and not current_split:\n",
    "                new_instances.append(new_instance)\n",
    "                instances_created += 1\n",
    "\n",
    "        return np.array(new_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ad4e11",
   "metadata": {},
   "source": [
    "## Explaination\n",
    "\n",
    "![](./trepan.png)\n",
    "\n",
    "### Initialization\n",
    "1. We define an empty queue.\n",
    "2. We predict all the labels for the data in X.\n",
    "3. We initialize the root node in the tree as a leaf node.\n",
    "4. We put a tuple (Node, training_examples, constraionts) into queue.\n",
    "\n",
    "### While-loop\n",
    "It runs as long as \n",
    "1. We still have less nodes than the tree_size_limit; or\n",
    "2. The queue is not empty\n",
    "\n",
    "While this is True:\n",
    "1. We pop a tuple from the queue, in a best first manner.\n",
    "Best-first expansion is different from traditional depth first expansion. In this case, if there are several nodes in the queue, it selects the one that yields the highest score from\n",
    "f(n) = reach(n) * (1 - fidelity(n)), where n is the node we evaluate, reach(n) is the fraction of original training examples that reach this node, and fidelity(n) is the estimated fidelity of the tree to the network for those instances.\n",
    "\n",
    "2. After we have identified the best tuple in the queue, we pop it, and name it N.\n",
    "We define the set of instances to N for examples_N, and the constraints at this point for constraints_N.\n",
    "\n",
    "3. Next, we identify all the possible splits in the dataset. For continuous data, we order the data, and set the split point in between two adjacent values in the dataset. This set is then shrinked by applying the constraints, so we remove illegal splits. In first iteration there are no constraints.\n",
    "\n",
    "4. We use the **oracle** to generate more instances that follows the constraints, and we label them.\n",
    "\n",
    "5. Next, we calculate the best initial binary split. That is, we use the set F_N and calculate which one has the best gain ratio.\n",
    "\n",
    "Gain Ratio is a modification of the Information Gain concept in decision tree algorithms, and it is used for handling the bias issue towards attributes with a large number of outcomes.\n",
    "\n",
    "Information Gain, a fundamental concept in decision tree algorithms, measures the reduction in entropy (uncertainty or impurity) achieved because of the split on a particular attribute. However, Information Gain has a bias towards attributes with many outcomes. That is, it tends to favor attributes that have a large number of distinct values over those with fewer distinct values. \n",
    "\n",
    "To overcome this bias, the Gain Ratio concept was introduced. The Gain Ratio is calculated by dividing the Information Gain by the so-called \"Intrinsic Value\" of a split. \n",
    "\n",
    "The Intrinsic Value measures the amount of information required to classify an instance given the split, without considering the class labels. It is calculated similarly to entropy but uses the proportions of the instances falling into each split subset instead of class proportions. \n",
    "\n",
    "By normalizing the Information Gain using the Intrinsic Value, Gain Ratio essentially penalizes attributes with many outcomes and reduces their bias. \n",
    "\n",
    "In conclusion, Gain Ratio is used to choose the attribute that gives the highest normalized reduction in entropy and is particularly useful for handling multi-valued attributes in decision tree algorithms.\n",
    "\n",
    "6. We now have the best split from the candidate set and its respective gain ratio score.\n",
    "\n",
    "7. Now, we use this as a the seed in a M-of-N split search. Here, N is the splitting criterias, while M is the number of those that need to be satisfied in order for the split to return true. A binary split is by definition a 1-of-1 split. The search expands by two operators:\n",
    "* m-of-(n+1) - we introduce a new splitting criteria without changing the threshold\n",
    "* (m+1)-of-(n+1) - we increase both the threshold and the splitting criterias.\n",
    "\n",
    "The search end when the number of N is equal to the number of features in the dataset.\n",
    "\n",
    "8. If the M-of-N search yields a better gain ratio than the binary split, we select that split criteria instead.\n",
    "\n",
    "9. We define our current node as a internal node instead of a leaf node.\n",
    "\n",
    "For [True, False] of our splitting critera:\n",
    "\n",
    "10. We define C, a child node of N, and define it as leaf node.\n",
    "\n",
    "11. We add the split as constraint for C\n",
    "\n",
    "12. We generate more instances that follows the constraints, and predict them.\n",
    "\n",
    "13. If we can say with some confidence that the majority of the labels are of one class, it we do not add it to the queue, and we designate the majority class label as the label on that leaf node.\n",
    "\n",
    "14. If there is no clear class majority, it creates a new tuple (C, examples_C, constraints_c) and adds it to queue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ace594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Node class\n",
    "class Node:\n",
    "    def __init__(self, training_examples, training_predictions, constraints, leaf, parent=None):\n",
    "        self.leaf = leaf\n",
    "        self.training_examples = training_examples\n",
    "        self.training_predictions = training_predictions\n",
    "        self.hard_constraints = constraints\n",
    "        self.split = None\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "        self.label = None\n",
    "        self.score = 0\n",
    "\n",
    "    def _is_leaf(self):\n",
    "        return len(self.children) == 0\n",
    "\n",
    "# Define the MofN class\n",
    "class MofN:\n",
    "    def __init__(self, m, conditions, gain_ratio = 0):\n",
    "        self.m = m\n",
    "        self.conditions = conditions\n",
    "        self.gain_ratio = gain_ratio\n",
    "        self.satisfied = True\n",
    "\n",
    "# Define the TREPAN class\n",
    "class TREPAN:\n",
    "    def __init__(self, oracle, X, y, max_tree_size, max_conditions, max_children, cutoff, num_of_instances):\n",
    "        self.oracle = oracle\n",
    "        self.root = None\n",
    "        self.features = X\n",
    "        self.X_true = X\n",
    "        self.y_true = y\n",
    "        self.y_predicted = self.oracle.model.predict(self.X_true) \n",
    "        self.max_tree_size = max_tree_size\n",
    "        self.max_conditions = max_conditions\n",
    "        self.max_children = max_children\n",
    "        self.current_amount_of_nodes = 0\n",
    "        self.cutoff = cutoff\n",
    "        self.S_min = len(self.X_true) // 10\n",
    "        self.number_of_instances_for_generation = num_of_instances\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for the given instances using the TREPAN decision tree.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): An array of instances for which class labels need to be predicted.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: An array of predicted class labels for the input instances.\n",
    "        \"\"\"\n",
    "\n",
    "        def traverse_tree(node, instance):\n",
    "            \"\"\"\n",
    "            Traverse the tree recursively to find the leaf node corresponding to the given instance.\n",
    "\n",
    "            Args:\n",
    "                node (Node): The current node being traversed.\n",
    "                instance (numpy.ndarray): A single instance for which a leaf node needs to be found.\n",
    "\n",
    "            Returns:\n",
    "                Node: The leaf node corresponding to the given instance.\n",
    "            \"\"\"\n",
    "\n",
    "            # If the current node is a leaf, return the node\n",
    "            if node.leaf:\n",
    "                return node\n",
    "\n",
    "            # If the current node is an internal node, traverse its children based on the m-of-n conditions\n",
    "            else:\n",
    "                for child in node.children:\n",
    "                    if self._satisfies_m_of_n_conditions(child.hard_constraints, instance):\n",
    "                        return traverse_tree(child, instance)\n",
    "\n",
    "\n",
    "        # Initialize an array to store predicted class labels\n",
    "        predictions = np.empty(X.shape[0], dtype=int)\n",
    "\n",
    "        # Iterate through the instances and predict class labels\n",
    "        for idx, instance in enumerate(X):\n",
    "            leaf_node = traverse_tree(self.root, instance)\n",
    "            predictions[idx] = leaf_node.label\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def fit(self):\n",
    "\n",
    "        # Define an empty queue\n",
    "        queue = []\n",
    "\n",
    "        # Use the oracle to predict y_predicted\n",
    "        # This happens in the init process of TREPAN\n",
    "\n",
    "        # Initialize root node as leaf\n",
    "        self.root = Node(self.X_true, self.y_predicted, [], True)\n",
    "\n",
    "        # Identify all possible candidate splits\n",
    "        F = self._identify_candidate_splits(self.features)\n",
    "\n",
    "        #Push node to queue\n",
    "        queue.append(self.root)\n",
    "\n",
    "        # Initialize best first expansion\n",
    "        self._best_first_tree_expansion(queue, F)\n",
    "\n",
    "    def _best_first_tree_expansion(self, queue, F):\n",
    "        \"\"\"\n",
    "        Expands the decision tree by selecting the best node and its corresponding split.\n",
    "        This function iteratively evaluates nodes in the queue, computes their gain ratio, and\n",
    "        generates child nodes until the maximum tree size is reached or the queue is empty.\n",
    "\n",
    "        Args:\n",
    "            queue (list): List of nodes to be evaluated and expanded.\n",
    "            F (list): List of candidate splits to be considered for each node.\n",
    "        \"\"\"\n",
    "\n",
    "        # 0. Run as long as there are still nodes in the queue and we havent reached limit\n",
    "        while queue and self.current_amount_of_nodes < self.max_tree_size:\n",
    "            \n",
    "            # 1. Evaluate which node in the queue has the highest score, and we pop this one\n",
    "            # The formula we use is f(n) = reach(n) * (1 - fidelity(n))\n",
    "            # Todo: Fix fidelity to compare towards tree.\n",
    "            N = self._get_best_scoring_node_in_queue(queue)\n",
    "            queue.remove(N)\n",
    "\n",
    "            # 2. Define F_N, which is the subset of all candidate splits which satisfies the current constraints\n",
    "            F_N = self._extract_subset_of_candidate_splits(F, N.hard_constraints)\n",
    "\n",
    "            # 3. Query the oracle to get more instances\n",
    "            X_from_oracle = self.oracle.generate_instances(N.hard_constraints, None, num_instances = self.number_of_instances_for_generation - (len(N.training_examples)))\n",
    "            y_from_oracle = self.oracle.model.predict(X_from_oracle)\n",
    "\n",
    "            # 4. Calculate gain ratio on all splits in F_N using gain ratio criterion\n",
    "            # Let best_initial_split be the top scoring candidate split in F_N using N.training_examples and X_from_oracle\n",
    "            best_initial_split, best_initial_gain_ratio = self._get_best_binary_split(F_N, N, X_from_oracle, y_from_oracle)\n",
    "\n",
    "            # 5. Convert it a m-of-n format, that is a 1-of-{best_initial_split}\n",
    "            best_binary_split = MofN(1, best_initial_split, best_initial_gain_ratio)\n",
    "\n",
    "            # 6. If the gain ratio = 1, then we already have a splitting condition which cannot be improved\n",
    "            # by a m-of-n search. Therefore, we only start the m-of-n search if we do not have a gain_ratio of 1.\n",
    "            if not best_binary_split.gain_ratio == 1:\n",
    "                \n",
    "                best_split = self._calculate_best_m_of_n_split(best_binary_split, F_N, N, X_from_oracle, y_from_oracle)\n",
    "                \n",
    "            # If there is a max gain initial split\n",
    "            else:\n",
    "                best_split = best_binary_split\n",
    "\n",
    "            # 8. Set current node as an internal node\n",
    "            N.leaf = False\n",
    "            N.split = best_split\n",
    "\n",
    "\n",
    "            # For every logical outcome of the m-of-n, we create a child node\n",
    "            #for new_constraints_c in best_split.outcomes:\n",
    "            for split_satisfied in [True, False]:\n",
    "                \n",
    "                # 11. Append constraints from parent node N\n",
    "                best_split.satisfied = split_satisfied\n",
    "                constraints_c = deepcopy(N.hard_constraints) + deepcopy([best_split])\n",
    "\n",
    "                # 12. Update original training data using the new constraints\n",
    "                child_mask = self._apply_m_of_n_constraints(constraints_c, N.training_examples)\n",
    "\n",
    "                # if there indeed is a split, and not one side is empty\n",
    "                if not (True in child_mask and False in child_mask):\n",
    "                    N.leaf = True\n",
    "                    break    \n",
    "\n",
    "                if split_satisfied:\n",
    "                    training_examples_c = deepcopy(N.training_examples[child_mask])\n",
    "                    training_predictions_c = deepcopy(N.training_predictions[child_mask])\n",
    "                else:\n",
    "                    training_examples_c = deepcopy(N.training_examples[~child_mask])\n",
    "                    training_predictions_c = deepcopy(N.training_predictions[~child_mask])\n",
    "\n",
    "                \n",
    "                # 13. Generate new set of instances for evaluation. The number is the defined number in init for evaluation minus the number from training examples.\n",
    "                if len(training_examples_c) < self.S_min:\n",
    "                    instances_for_evaluation = self.oracle.generate_instances(N.hard_constraints, best_split, self.S_min - (len(training_examples_c)))\n",
    "                else:\n",
    "                    instances_for_evaluation = np.array([])\n",
    "\n",
    "                # 14. Create a new child node as leaf node, define it as child of N, and add to node count (for stopping criteria).\n",
    "                if training_examples_c.size != 0:\n",
    "                    C = Node(training_examples_c, training_predictions_c, constraints_c, True, parent=N)\n",
    "    \n",
    "                    N.children.append(C)\n",
    "                    self.current_amount_of_nodes += 1\n",
    "\n",
    "                    # 15. Get the most common class prediction using the oracle\n",
    "                    most_common_class, p_c = self._most_common_class_proportion(instances_for_evaluation, training_examples_c, training_predictions_c)\n",
    "                    \n",
    "                    # 16. If proportion is larger than some cut-off value, let it be a leaf and assign target class\n",
    "                    if p_c >= self.cutoff:\n",
    "                        C.label = most_common_class\n",
    "                    \n",
    "                    # Otherwise, append child node to the queue.\n",
    "                    else:\n",
    "                        C.label = most_common_class\n",
    "                        queue.append(C)\n",
    "\n",
    "        if queue:\n",
    "            for node in queue:\n",
    "                most_common_class, p_c = self._most_common_class_proportion(np.array([]), node.training_examples, node.training_predictions)\n",
    "                node.label = most_common_class\n",
    "\n",
    "    def _identify_candidate_splits(self, X):\n",
    "        \"\"\"\n",
    "        Identify all possible candidate splits for all features in the input feature matrix X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            The input feature matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        candidate_splits : list of tuples\n",
    "            A list of candidate splits, where each split is represented as a tuple (feature_index, threshold).\n",
    "        \"\"\"\n",
    "        # Get the number of instances and features\n",
    "        num_instances, num_features = X.shape\n",
    "        \n",
    "        # Initialize an empty list to store candidate splits\n",
    "        candidate_splits = []\n",
    "\n",
    "        # Iterate over each feature column in the input matrix X\n",
    "        for col in range(num_features):\n",
    "            # Find the unique values for the current feature\n",
    "            unique_values = np.unique(X[:, col])\n",
    "            \n",
    "            # If there's only one unique value, continue to the next feature\n",
    "            if len(unique_values) == 1:\n",
    "                continue\n",
    "\n",
    "            # If there are more than two unique values, calculate thresholds as the midpoint between adjacent unique values\n",
    "            if len(unique_values) > 2:\n",
    "                thresholds = (unique_values[:-1] + unique_values[1:]) / 2.0\n",
    "            else:\n",
    "                thresholds = unique_values\n",
    "\n",
    "            # Append the feature index and candidate split threshold to the list of candidate splits\n",
    "            for threshold in thresholds:\n",
    "                candidate_splits.append((col, round(threshold, 2), '<='))\n",
    "                candidate_splits.append((col, round(threshold, 2), '>'))\n",
    "\n",
    "        return candidate_splits\n",
    "\n",
    "    def _get_best_binary_split(self, F_N, node, X_from_oracle, y_from_oracle):\n",
    "        best_gain_ratio = -1\n",
    "        best_candidate_split = None\n",
    "        X = np.vstack((node.training_examples, X_from_oracle))\n",
    "        y = np.concatenate((node.training_predictions, y_from_oracle))\n",
    "\n",
    "        for candidate in F_N:\n",
    "            gain_ratio = self._calculate_gain_ratio(X, y, [candidate])\n",
    "\n",
    "            if gain_ratio > best_gain_ratio:\n",
    "                best_gain_ratio = gain_ratio\n",
    "                best_candidate_split = [candidate]\n",
    "\n",
    "        return best_candidate_split, best_gain_ratio\n",
    "                \n",
    "    def _calculate_best_m_of_n_split(self, best_binary_split, F_N, node, X_from_oracle, y_from_oracle):\n",
    "        X = np.vstack((node.training_examples, X_from_oracle))\n",
    "        y = np.concatenate((node.training_predictions, y_from_oracle))\n",
    "\n",
    "        best_m_of_n_split = best_binary_split\n",
    "        current_conditions = list(best_binary_split.conditions)\n",
    "\n",
    "        def condition_exists(conditions, condition):\n",
    "            for cond in conditions:\n",
    "                if cond[0] == condition[0] and cond[2] == condition[2]:\n",
    "                    return True\n",
    "            return False\n",
    "    \n",
    "        while len(current_conditions) < self.max_conditions:\n",
    "            # Find the best condition to add from F_N\n",
    "            best_new_condition = None\n",
    "            best_gain_ratio = 0\n",
    "            best_m = deepcopy(best_m_of_n_split.m)\n",
    "\n",
    "            for candidate in F_N:\n",
    "                if candidate not in current_conditions and not condition_exists(current_conditions, candidate):\n",
    "                    # Try adding the candidate to the conditions\n",
    "                    extended_conditions = current_conditions + [candidate]\n",
    "\n",
    "                    # Calculate gain ratio for m-of-n+1\n",
    "                    gain_ratio_m_of_n_plus_1 = self._calculate_gain_ratio_m_of_n(X, y, extended_conditions, best_m)\n",
    "                    # Calculate gain ratio for m+1-of-n+1\n",
    "                    gain_ratio_m_plus_1_of_n_plus_1 = self._calculate_gain_ratio_m_of_n(X, y, extended_conditions, best_m + 1)\n",
    "\n",
    "                    if gain_ratio_m_plus_1_of_n_plus_1 > gain_ratio_m_of_n_plus_1:\n",
    "                        candidate_gain_ratio = gain_ratio_m_plus_1_of_n_plus_1\n",
    "                        candidate_m = best_m + 1\n",
    "                    else:\n",
    "                        candidate_gain_ratio = gain_ratio_m_of_n_plus_1\n",
    "                        candidate_m = best_m\n",
    "\n",
    "                    #decimal error fix. COuld be 1.00000002 for instance.\n",
    "                    if candidate_gain_ratio > 1:\n",
    "                        candidate_gain_ratio = 1\n",
    "\n",
    "                    # To make sure that the added complexity added by increasing the search has some substantial increase in gain, we add a minimum increase of 0.01.\n",
    "                    if (candidate_gain_ratio - 0.01) > best_gain_ratio:\n",
    "                        best_new_condition = candidate\n",
    "                        best_gain_ratio = candidate_gain_ratio\n",
    "                        best_m = candidate_m\n",
    "\n",
    "            # Add the best new condition to the current conditions\n",
    "            if best_new_condition is not None:\n",
    "                current_conditions.append(best_new_condition)\n",
    "\n",
    "                # Update the best_m_of_n_split with the new conditions and gain ratio\n",
    "                if best_gain_ratio > best_m_of_n_split.gain_ratio:\n",
    "                    best_m_of_n_split = MofN(best_m, current_conditions, best_gain_ratio)\n",
    "\n",
    "            # Break the loop if the gain ratio is 1.0 or if the number of children exceeds the maximum allowed\n",
    "            if best_gain_ratio == 1.0 or self._get_child_count_for_MofN_structure(best_m, len(current_conditions)) >= self.max_children:\n",
    "                break\n",
    "\n",
    "        return best_m_of_n_split\n",
    "\n",
    "    def _apply_constraints(self, constraints, X):\n",
    "        # Create a boolean mask with the same length as the number of instances in X, initialized with True values\n",
    "        mask = np.ones(len(X), dtype=bool)\n",
    "\n",
    "        # Iterate over each constraint in the list of constraints\n",
    "        for col, threshold, direction in constraints:\n",
    "            if direction == '<=':\n",
    "                # Create a boolean mask for instances where the feature value at column 'col' is less than or equal to the threshold\n",
    "                current_mask = X[:, col] <= threshold\n",
    "            \n",
    "            elif direction == '>':\n",
    "                current_mask = X[:, col] > threshold\n",
    "            \n",
    "            else:\n",
    "                raise SyntaxError(\"Wrong syntax for direction in tuple. Should be '<=' or '>'.\")\n",
    "            \n",
    "            # Apply the current mask to the main mask using the AND operation\n",
    "            mask = mask & current_mask\n",
    "\n",
    "        # Return the mask\n",
    "        return mask\n",
    "    \n",
    "    def _apply_m_of_n_constraints(self, mofn_split_list, X):\n",
    "        overall_mask = np.ones(len(X), dtype=bool)\n",
    "\n",
    "        # Iterate over each MofN split object in the list\n",
    "        for mofn_split in mofn_split_list:\n",
    "            m = mofn_split.m\n",
    "            conditions = mofn_split.conditions\n",
    "\n",
    "            # Initialize an array to count the number of satisfied conditions for each instance\n",
    "            satisfied_conditions_count = np.zeros(len(X), dtype=int)\n",
    "\n",
    "            for feature_idx, threshold_value, less_than_or_greater in conditions:\n",
    "                if less_than_or_greater == '<=':\n",
    "                    satisfied_conditions_count += (X[:, feature_idx] <= threshold_value)\n",
    "                else:\n",
    "                    satisfied_conditions_count += (X[:, feature_idx] > threshold_value)\n",
    "\n",
    "            # Get the mask for instances satisfying the m-of-n split\n",
    "            m_of_n_mask = satisfied_conditions_count >= m\n",
    "            overall_mask &= m_of_n_mask\n",
    "\n",
    "        return overall_mask\n",
    "\n",
    "\n",
    "    def _get_child_count_for_MofN_structure(self, m, n):\n",
    "        # From statistics, we are familiar with nCr, which is how many combinations of n items where r is selected, \n",
    "        # when order does not matter and repetitions are not allowed. We can use this here to calculate how \n",
    "        # many children a certain m-of-n structure will produce. Note that now, n is a number and not a condition for this calculation.\n",
    "        # In our case, it will be nCm. For instance a 2-of-{A, B, C} will be a 2-of-3 structure with outcomes {A, B}, {A, C} and {B, C}\n",
    "        # The number of children can be calculated: nCm = (n!) / ((n-m)! * m!)\n",
    "        return ((math.factorial(n)) / (math.factorial(n-m) * math.factorial(m)))\n",
    "    \n",
    "    def _get_best_scoring_node_in_queue(self, queue):\n",
    "        best_score = float('-inf')\n",
    "        best_node = None\n",
    "\n",
    "        for node in queue:\n",
    "            score = self._calculate_node_score(node)\n",
    "            node.score = score\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_node = node\n",
    "        \n",
    "        return best_node\n",
    "\n",
    "    def _calculate_node_score(self, node):\n",
    "        \"\"\"\n",
    "        Calculate the score for a given node.\n",
    "        \n",
    "        !! This doesnt work yet !!\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node : TrepanNode\n",
    "            The node for which to calculate the score.\n",
    "        X_true : numpy.ndarray\n",
    "            The input feature matrix.\n",
    "        y_true : numpy.ndarray\n",
    "            The true labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            The node score.\n",
    "        \"\"\"\n",
    "        # Apply the constraints of the node\n",
    "        mask = self._apply_m_of_n_constraints(node.hard_constraints, node.training_examples)\n",
    "\n",
    "        # Apply mask\n",
    "        X_filtered = node.training_examples[mask]\n",
    "        y_filtered = node.training_predictions[mask]\n",
    "\n",
    "        # Calculate the fidelity\n",
    "        y_pred = self.oracle.model.predict(X_filtered)\n",
    "        fidelity = self._calculate_fidelity(y_filtered, y_pred)\n",
    "\n",
    "        # Calculate the reach\n",
    "        reach = self._calculate_reach(node.hard_constraints)\n",
    "\n",
    "        # Calculate the node score\n",
    "        score = reach * fidelity\n",
    "\n",
    "        if math.isnan(score):\n",
    "            return 0\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def _calculate_fidelity(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the fidelity of the predicted values compared to the true values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : numpy.ndarray\n",
    "            The true labels.\n",
    "        y_pred : numpy.ndarray\n",
    "            The predicted labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        fidelity : float\n",
    "            The fidelity score.\n",
    "        \"\"\"\n",
    "        assert y_true.shape == y_pred.shape, \"y_true and y_pred should have the same shape.\"\n",
    "        return np.mean(y_true == y_pred)\n",
    "\n",
    "    def _calculate_reach(self, constraints):\n",
    "        \"\"\"\n",
    "        Calculate the reach of a given set of instances given a set of constraints.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_true : numpy.ndarray\n",
    "            The input feature matrix.\n",
    "        constraints : list of tuples\n",
    "            The constraints to apply.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        reach : float\n",
    "            The reach score.\n",
    "        \"\"\"\n",
    "        num_instances, _ = self.X_true.shape\n",
    "        num_instances_satisfying_constraints = 0\n",
    "\n",
    "        if not constraints:\n",
    "            return 1\n",
    "\n",
    "        for instance in self.X_true:\n",
    "            satisfies_constraints = True\n",
    "            for constraint in constraints:\n",
    "                    m = constraint.m\n",
    "                    conditions = constraint.conditions\n",
    "                    satisfied_conditions_count = 0\n",
    "\n",
    "                    for feature_idx, threshold, direction in conditions:\n",
    "                        if direction == \"<=\" and instance[feature_idx] <= threshold:\n",
    "                            satisfied_conditions_count += 1\n",
    "                            #satisfies_constraints = False\n",
    "                            #break\n",
    "                        if direction == \">\" and instance[feature_idx] > threshold:\n",
    "                            satisfied_conditions_count += 1\n",
    "                            #satisfies_constraints = False\n",
    "                            #break\n",
    "                    \n",
    "                    if satisfied_conditions_count < m:\n",
    "                        satisfies_constraints = False\n",
    "                        break\n",
    "\n",
    "            if satisfies_constraints:\n",
    "                    num_instances_satisfying_constraints += 1\n",
    "\n",
    "        reach = num_instances_satisfying_constraints / num_instances\n",
    "        return reach\n",
    "    \n",
    "    def _most_common_class_proportion(self, X_from_oracle, training_examples_c, training_predictions_c):\n",
    "        # Predict the targets on the instances\n",
    "        if X_from_oracle.size > 0:\n",
    "            y_pred = self.oracle.model.predict(X_from_oracle)\n",
    "            y = np.concatenate((training_predictions_c, y_pred))\n",
    "        else:\n",
    "            y = training_predictions_c    \n",
    "                #y = training_predictions_c\n",
    "        #while len(y) < self.S_min:\n",
    "\n",
    "        if y.size == 0:\n",
    "            return None\n",
    "        \n",
    "        # Count the occurrences of each class\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "        # Find the index of the most common class\n",
    "        most_common_class_idx = np.argmax(counts)\n",
    "\n",
    "        # Calculate the proportion of the most common class\n",
    "        most_common_class = unique_classes[most_common_class_idx]\n",
    "        proportion = counts[most_common_class_idx] / y.size\n",
    "\n",
    "        return most_common_class, proportion\n",
    "\n",
    "    def _extract_subset_of_candidate_splits(self, F, constraints):\n",
    "        \"\"\"\n",
    "        Extracts the subset of candidate splits that satisfy the list of constraints.\n",
    "\n",
    "        :param F: list of candidate splits (tuples of structure (feature_idx, threshold))\n",
    "        :param constraints: list of MofN constraints\n",
    "        :return: list of candidate splits that satisfy the constraints\n",
    "        \"\"\"\n",
    "\n",
    "        if not constraints:\n",
    "            return F\n",
    "\n",
    "        def satisfies_constraints(candidate_split):\n",
    "            feature_idx, threshold, direction = candidate_split\n",
    "\n",
    "            for constraint in constraints:\n",
    "                constraint_conditions = constraint.conditions\n",
    "\n",
    "                for constraint_feature_idx, constraint_threshold, constraint_direction in constraint_conditions:\n",
    "                    if feature_idx == constraint_feature_idx:\n",
    "                        if constraint_direction == \"<=\" and threshold > constraint_threshold:\n",
    "                            return False\n",
    "                        elif constraint_direction == \">\" and threshold <= constraint_threshold:\n",
    "                            return False\n",
    "\n",
    "            return True\n",
    "\n",
    "        result = [candidate_split for candidate_split in F if satisfies_constraints(candidate_split)]\n",
    "        return result\n",
    "\n",
    "    def _satisfies_m_of_n_conditions(self, mofn_constraints, instance):\n",
    "        \"\"\"\n",
    "        Check if a given instance satisfies the m-of-n conditions of a specific node.\n",
    "\n",
    "        Args:\n",
    "        mofn_constraints (list of MofN objects): List of MofN constraints.\n",
    "        instance (numpy array): The instance we want to check against the constraints.\n",
    "\n",
    "        Returns:\n",
    "        bool: True if the instance satisfies the m-of-n conditions, False otherwise.\n",
    "        \"\"\"\n",
    "\n",
    "        all_constraints_satisfied = True\n",
    "\n",
    "        for mofn_constraint in mofn_constraints:\n",
    "            m = mofn_constraint.m\n",
    "            conditions = mofn_constraint.conditions\n",
    "            satisfied_conditions_count = 0\n",
    "\n",
    "            for feature_idx, threshold, direction in conditions:\n",
    "                if direction == \"<=\":\n",
    "                    if instance[feature_idx] <= threshold:\n",
    "                        satisfied_conditions_count += 1\n",
    "                elif direction == \">\":\n",
    "                    if instance[feature_idx] > threshold:\n",
    "                        satisfied_conditions_count += 1\n",
    "\n",
    "            if satisfied_conditions_count >= m:\n",
    "                constraint_satisfied = mofn_constraint.satisfied\n",
    "            else:\n",
    "                constraint_satisfied = not mofn_constraint.satisfied\n",
    "\n",
    "            all_constraints_satisfied = all_constraints_satisfied and constraint_satisfied\n",
    "\n",
    "        return all_constraints_satisfied\n",
    "\n",
    "    def _calculate_gain_ratio(self, X, y, constraints):\n",
    "        \n",
    "        if not constraints:\n",
    "            mask = np.ones(len(X), dtype=bool)\n",
    "        else: \n",
    "            mask = self._apply_constraints(constraints, X)\n",
    "        \n",
    "        y_entropy = self._calculate_entropy(y)\n",
    "\n",
    "        y_left = y[mask]\n",
    "        y_right = y[~mask]\n",
    "\n",
    "        # Calculate the entropy for both splits\n",
    "        left_entropy = self._calculate_entropy(y_left)\n",
    "        right_entropy = self._calculate_entropy(y_right)\n",
    "\n",
    "        # Calculate the weighted average of the entropies\n",
    "        left_weight = len(y_left) / len(y)\n",
    "        right_weight = len(y_right) / len(y)\n",
    "        avg_entropy = left_weight * left_entropy + right_weight * right_entropy\n",
    "\n",
    "        # Calculate information gain\n",
    "        info_gain = y_entropy - avg_entropy\n",
    "\n",
    "        # Calculate intrinsic value\n",
    "        intrinsic_value = -left_weight * np.log2(left_weight) - right_weight * np.log2(right_weight) if left_weight > 0 and right_weight > 0 else 0\n",
    "\n",
    "        # Calculate gain ratio\n",
    "        gain_ratio = info_gain / intrinsic_value if intrinsic_value != 0 else 0\n",
    "\n",
    "        return gain_ratio\n",
    "\n",
    "    def _calculate_gain_ratio_m_of_n(self, X, y, new_proposed_conditions, m):\n",
    "        # Calculate the entropy of the parent node before splitting\n",
    "        parent_entropy = self._calculate_entropy(y)\n",
    "\n",
    "        gain_ratios = []\n",
    "\n",
    "        # Iterate over each logical outcome of the m-of-n split using itertools.combinations\n",
    "        for outcome in combinations(new_proposed_conditions, m):\n",
    "            new_constraints = list(outcome)\n",
    "            weighted_entropy = 0\n",
    "            intrinsic_value = 0\n",
    "\n",
    "            # Apply the constraints to the dataset\n",
    "            child_mask = self._apply_constraints(new_constraints, X)\n",
    "            child_examples = X[child_mask]\n",
    "            child_predictions = y[child_mask]\n",
    "            not_applicable = X[~child_mask]\n",
    "            not_applicable_predictions = y[~child_mask]\n",
    "\n",
    "            # Calculate the entropy of the resulting child node\n",
    "            child_entropy = self._calculate_entropy(child_predictions)\n",
    "            not_child_entropy = self._calculate_entropy(not_applicable_predictions)\n",
    "\n",
    "            # Calculate the proportion of instances that fall into the child node\n",
    "            child_proportion = len(child_examples) / len(X)\n",
    "\n",
    "            # Update the weighted child entropy sum and intrinsic value sum\n",
    "            weighted_entropy += ((child_proportion * child_entropy) + ((1 - child_proportion) * not_child_entropy))\n",
    "            intrinsic_value -= ((child_proportion * np.log2(child_proportion)) + ((1 - child_proportion) * np.log2(1 - child_proportion)))\n",
    "        \n",
    "            # Calculate the information gain and gain ratio\n",
    "            information_gain = parent_entropy - weighted_entropy\n",
    "            gain_ratio = information_gain / intrinsic_value\n",
    "        \n",
    "            gain_ratios.append(gain_ratio)\n",
    "\n",
    "        return np.mean(gain_ratios)\n",
    "\n",
    "    def _calculate_intrinsic_value(self, y):\n",
    "        num_instances = len(y)\n",
    "        unique_labels, label_counts = np.unique(y, return_counts=True)\n",
    "        proportions = label_counts / num_instances\n",
    "        \n",
    "        # Calculate the intrinsic value (split information)\n",
    "        intrinsic_value = -np.sum(proportions * np.log2(proportions))\n",
    "        \n",
    "        return intrinsic_value\n",
    "\n",
    "    def _calculate_entropy(self, y):\n",
    "            num_instances = len(y)\n",
    "            unique_labels, label_counts = np.unique(y, return_counts=True)\n",
    "            probabilities = label_counts / num_instances\n",
    "            entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "            return entropy\n",
    "    \n",
    "    def print_tree(self, node=None, level=0):\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "\n",
    "        # Print the current node\n",
    "        if node.leaf:\n",
    "            print(\"  \" * level + f\"Leaf (Class: {node.label}, Constraints: \", end=\"\")\n",
    "            for mofn in node.hard_constraints:\n",
    "                satisfaction = \"Satisfied\" if mofn.satisfied else \"Not Satisfied\"\n",
    "                print(f\"{satisfaction} {mofn.m}-of-{[condition for condition in mofn.conditions]}\", end=\", \")\n",
    "            print(\")\")\n",
    "\n",
    "        else:\n",
    "            print(\"  \" * level + f\"Node (Split: {node.split.m}-of-{[condition for condition in node.split.conditions]}, Gain Ratio: {node.split.gain_ratio:.2f})\")\n",
    "\n",
    "        # Recursively print the children\n",
    "        for child in node.children:\n",
    "            self.print_tree(child, level + 1)\n",
    "\n",
    "\n",
    "    def to_graphviz(self, node=None, graph=None, parent=None):\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "\n",
    "        if graph is None:\n",
    "            graph = Digraph(\"TREPAN_Tree\", format=\"png\")\n",
    "            graph.attr(rankdir=\"TB\")\n",
    "\n",
    "        node_id = f\"{id(node)}\"\n",
    "        if node.leaf:\n",
    "            constraints_str = ',\\n'.join([f\"{('Satisfied' if mofn.satisfied else 'Not Satisfied')} {mofn.m}-of-\\n{[condition for condition in mofn.conditions]}\" for mofn in node.hard_constraints])\n",
    "            label = f\"Leaf\\n(Class: {node.label},\\nConstraints:\\n{constraints_str})\"\n",
    "        else:\n",
    "            label = f\"Node\\n(Split: {node.split.m}-of-\\n{[condition for condition in node.split.conditions]},\\nGain Ratio: {node.split.gain_ratio:.2f})\"\n",
    "\n",
    "        graph.node(node_id, label=label)\n",
    "\n",
    "        if parent is not None:\n",
    "            graph.edge(f\"{id(parent)}\", node_id)\n",
    "\n",
    "        for child in node.children:\n",
    "            self.to_graphviz(child, graph, node)\n",
    "\n",
    "        return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45db58ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anders/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/anders/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/tmp/ipykernel_1025554/1695751300.py:655: RuntimeWarning: divide by zero encountered in log2\n",
      "  intrinsic_value -= ((child_proportion * np.log2(child_proportion)) + ((1 - child_proportion) * np.log2(1 - child_proportion)))\n",
      "/tmp/ipykernel_1025554/1695751300.py:655: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  intrinsic_value -= ((child_proportion * np.log2(child_proportion)) + ((1 - child_proportion) * np.log2(1 - child_proportion)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaf (Class: None, Constraints: )\n",
      "Image successfully generated\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage successfully generated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m y_test_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtrepan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Accuracy\u001b[39;00m\n\u001b[1;32m     22\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_test_pred)\n",
      "Cell \u001b[0;32mIn[5], line 82\u001b[0m, in \u001b[0;36mTREPAN.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, instance \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X):\n\u001b[1;32m     81\u001b[0m     leaf_node \u001b[38;5;241m=\u001b[39m traverse_tree(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, instance)\n\u001b[0;32m---> 82\u001b[0m     predictions[idx] \u001b[38;5;241m=\u001b[39m leaf_node\u001b[38;5;241m.\u001b[39mlabel\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "# TREPAN\n",
    "oracle = Oracle(model, X_train, y_train)\n",
    "\n",
    "#Parameters\n",
    "max_number_of_nodes = 15\n",
    "number_of_instances, max_conditions = X.shape\n",
    "S_min = number_of_instances // 10\n",
    "max_children_per_node = 5\n",
    "proportion_to_determine_class_in_leaf_node = 0.70\n",
    "\n",
    "trepan = TREPAN(oracle=oracle, X=X_train, y=y_train, max_tree_size=max_number_of_nodes, max_conditions=max_conditions, max_children=max_children_per_node, cutoff=proportion_to_determine_class_in_leaf_node, num_of_instances=number_of_instances)\n",
    "trepan.fit()\n",
    "trepan.print_tree()\n",
    "graph = trepan.to_graphviz()\n",
    "graph.render(\"trepan_tree\", cleanup=True)\n",
    "print(\"Image successfully generated\")\n",
    "\n",
    "# Predictions\n",
    "y_test_pred = trepan.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Accuracy Score:\", accuracy)\n",
    "\n",
    "# Confusion matrix\n",
    "confusion_mat_2 = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat_2)\n",
    "\n",
    "# Visualize the confusion matrix using a heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(confusion_mat_2, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "#Fidelity\n",
    "accuracy = accuracy_score(y_pred, y_test_pred)\n",
    "print(\"Fidelity Score:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdd8928",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd9c3ad3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
